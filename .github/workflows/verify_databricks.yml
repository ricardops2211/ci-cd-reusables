name: verify-databricks

on:
  workflow_call:
    inputs:
      databricks_host:        { type: string, required: true }
      notebooks_dir:          { type: string, default: "notebooks" }
      data_dir:               { type: string, default: "data" }
      raw_clientes:           { type: string, default: "data/raw/clientes.csv" }
      raw_ventas:             { type: string, default: "data/raw/ventas.json" }
      config_param_file:      { type: string, default: "data/config/parametros.yaml" }
      jobs_dir:               { type: string, default: "jobs" }
      python_scripts_dir:     { type: string, default: "scripts" }
      outputs_dbfs:           { type: string, default: "dbfs:/FileStore/jobs_output/" }
      artifact_name:          { type: string, default: "job-outputs" }
      retention_days:         { type: number, default: 7 }
    secrets:
      DATABRICKS_TOKEN:
        required: true

jobs:
  verify:
    runs-on: self-hosted
    env:
      DATABRICKS_HOST: ${{ inputs.databricks_host }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
    steps:
      - uses: actions/checkout@v4

      - name: A√±adir venv de Databricks al PATH (si existe)
        shell: bash
        run: |
          if [ -d "$HOME/databricks-env/bin" ]; then
            echo "$HOME/databricks-env/bin" >> "$GITHUB_PATH"
          fi

      - name: Importar notebooks en Databricks
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"

          target_ws_dir="/Shared/notebook/py"
          databricks workspace mkdirs "$target_ws_dir"

          shopt -s nullglob
          for nb in "${{ inputs.notebooks_dir }}"/*.py; do
            base="$(basename "$nb")"
            echo "Importando $base en ${target_ws_dir}..."
            databricks workspace import "$nb" "${target_ws_dir}/${base}" \
              --language PYTHON --overwrite
          done

      - name: Subir archivos de data y config a DBFS
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"

          databricks fs mkdirs dbfs:/FileStore/data
          databricks fs mkdirs dbfs:/FileStore/jobs_data/raw
          databricks fs mkdirs dbfs:/FileStore/jobs_data/config

          shopt -s nullglob
          for p in "${{ inputs.data_dir }}"/*; do
            name="$(basename "$p")"
            if [ -d "$p" ]; then
              echo "üì§ Carpeta $name ‚Üí DBFS:/FileStore/data/$name"
              databricks fs cp -r "$p" "dbfs:/FileStore/data/$name" --overwrite
            else
              echo "üì§ Archivo $name ‚Üí DBFS:/FileStore/data/$name"
              databricks fs cp "$p" "dbfs:/FileStore/data/$name" --overwrite
            fi
          done

          databricks fs cp "${{ inputs.raw_clientes }}" dbfs:/FileStore/jobs_data/raw/clientes.csv --overwrite
          databricks fs cp "${{ inputs.raw_ventas }}"  dbfs:/FileStore/jobs_data/raw/ventas.json  --overwrite
          databricks fs cp "${{ inputs.config_param_file }}" dbfs:/FileStore/jobs_data/config/parametros.yaml --overwrite

          echo "üìÇ Contenido de dbfs:/FileStore/data"
          databricks fs ls dbfs:/FileStore/data || true
          echo "üìÇ Contenido de dbfs:/FileStore/jobs_data/raw"
          databricks fs ls dbfs:/FileStore/jobs_data/raw || true
          echo "üìÇ Contenido de dbfs:/FileStore/jobs_data/config"
          databricks fs ls dbfs:/FileStore/jobs_data/config || true

      - name: Verificar archivos subidos a DBFS
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"

          PARAM_FILE="dbfs:/FileStore/jobs_data/config/parametros.yaml"
          if databricks fs head "$PARAM_FILE" -n 1 >/dev/null 2>&1; then
            echo "‚úÖ parametros.yaml existe en DBFS"
          else
            echo "‚ùå parametros.yaml NO existe en DBFS"
            exit 1
          fi

      - name: Crear/Actualizar Job(s) en Databricks (UPSERT)
        shell: bash
        env:
          JOBS_DIR: ${{ inputs.jobs_dir }}
          JOB_IDS_PATH: .gha/job_ids.json
        run: |
          set -Eeuo pipefail
          mkdir -p .gha
          python3 "${{ inputs.python_scripts_dir }}/create-jobdatabricks.py"

      - name: Actualizacion de joblist.tmp
        shell: bash
        env:
          WRITE_MAPPING: "false"
          WRITE_FULL_MAPPING: "true"
          FILTER_REPO_ONLY: "true"
        run: |
          set -Eeuo pipefail
          python3 "${{ inputs.python_scripts_dir }}/joblisttmp.py"

      - name: Ejecutar Job(s) y verificar
        shell: bash
        env:
          JOB_IDS_PATH: .gha/job_ids.json
          POLL_SECONDS: "10"
          TAIL_INTERVAL: "10"
          TIMEOUT_SECONDS: "7200"
          LOGS_DBFS_BASE: "dbfs:/cluster-logs"
        run: |
          set -Eeuo pipefail
          python3 "${{ inputs.python_scripts_dir }}/execute-jobdatabricks.py"