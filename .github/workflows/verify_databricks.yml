name: verify-databricks

on:
  workflow_call:
    inputs:
      databricks_host:    { type: string,  required: true }
      notebooks_dir:      { type: string,  default: "notebooks" }
      data_dir:           { type: string,  default: "data" }
      raw_clientes:       { type: string,  default: "data/raw/clientes.csv" }
      raw_ventas:         { type: string,  default: "data/raw/ventas.json" }
      config_param_file:  { type: string,  default: "data/config/parametros.yaml" }
      jobs_dir:           { type: string,  default: "jobs" }
      python_scripts_dir: { type: string,  default: "scripts" }
      outputs_dbfs:       { type: string,  default: "dbfs:/FileStore/jobs_output/" }
      artifact_name:      { type: string,  default: "job-outputs" }
      retention_days:     { type: number,  default: 7 }
    secrets:
      DATABRICKS_TOKEN_B64: { required: true }
      # REUSABLES_TOKEN: { required: false }  # <-- solo si el repo reusable es privado

jobs:
  verify:
    runs-on: self-hosted
    env:
      DATABRICKS_HOST: ${{ inputs.databricks_host }}
    steps:
      - uses: actions/checkout@v4   # Esto clona el REPO DEL CALLER por defecto. :contentReference[oaicite:2]{index=2}

      - name: Checkout del repo del reusable (para traer scripts)
        uses: actions/checkout@v4
        with:
          repository: ricardops2211/ci-cd-reusables
          ref: databricks
          path: _reusables
          # token: ${{ secrets.REUSABLES_TOKEN }}   # <-- descomenta si el repo reusable es PRIVADO

      - name: A√±adir venv de Databricks al PATH (si existe)
        shell: bash
        run: |
          if [ -d "$HOME/databricks-env/bin" ]; then
            echo "$HOME/databricks-env/bin" >> "$GITHUB_PATH"
          fi

      - name: Decodificar token y exportar a env
        shell: bash
        run: |
          set -Eeuo pipefail
          DBX_TOKEN="$(printf %s "${{ secrets.DATABRICKS_TOKEN_B64 }}" | base64 -d | base64 -d)"
          if [ -z "$DBX_TOKEN" ]; then
            echo "‚ùå DATABRICKS_TOKEN vac√≠o tras decodificar"; exit 1
          fi
          echo "::add-mask::$DBX_TOKEN"
          echo "DATABRICKS_TOKEN=$DBX_TOKEN" >> "$GITHUB_ENV"

      - name: Sanity check de token
        shell: bash
        run: |
          test -n "$DATABRICKS_TOKEN" || { echo "‚ùå Token no presente"; exit 1; }

      - name: Resolver ruta de scripts (create/execute/joblist)
        id: resolve_scripts
        shell: bash
        run: |
          set -Eeuo pipefail
          # 1) Intento directo con el input
          CAND="${{ inputs.python_scripts_dir }}"
          if [ -f "$CAND/create-jobdatabricks.py" ] && \
             [ -f "$CAND/execute-jobdatabricks.py" ] && \
             [ -f "$CAND/joblisttmp.py" ]; then
            echo "CREATE_SCRIPT=$CAND/create-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "EXECUTE_SCRIPT=$CAND/execute-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "JOBLIST_SCRIPT=$CAND/joblisttmp.py"         >> "$GITHUB_ENV"
            exit 0
          fi

          # 2) Fallback: scripts dentro del repo del reusable
          if [ -f "_reusables/scripts/create-jobdatabricks.py" ] && \
             [ -f "_reusables/scripts/execute-jobdatabricks.py" ] && \
             [ -f "_reusables/scripts/joblisttmp.py" ]; then
            echo "CREATE_SCRIPT=_reusables/scripts/create-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "EXECUTE_SCRIPT=_reusables/scripts/execute-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "JOBLIST_SCRIPT=_reusables/scripts/joblisttmp.py" >> "$GITHUB_ENV"
            exit 0
          fi

          # 3) B√∫squeda global como √∫ltimo recurso
          CREATE="$(find . -maxdepth 6 -type f -name create-jobdatabricks.py | head -n1 || true)"
          EXECUTE="$(find . -maxdepth 6 -type f -name execute-jobdatabricks.py | head -n1 || true)"
          JOBLIST="$(find . -maxdepth 6 -type f -name joblisttmp.py | head -n1 || true)"

          if [ -n "$CREATE" ] && [ -n "$EXECUTE" ] && [ -n "$JOBLIST" ]; then
            echo "CREATE_SCRIPT=$CREATE"   >> "$GITHUB_ENV"
            echo "EXECUTE_SCRIPT=$EXECUTE" >> "$GITHUB_ENV"
            echo "JOBLIST_SCRIPT=$JOBLIST" >> "$GITHUB_ENV"
          else
            echo "‚ùå No se encontraron scripts requeridos."
            echo "Busqu√© en '${{ inputs.python_scripts_dir }}', en '_reusables/scripts' y en todo el repo."
            exit 1
          fi

      - name: Importar notebooks en Databricks
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"
          target_ws_dir="/Shared/notebook/py"
          databricks workspace mkdirs "$target_ws_dir"
          shopt -s nullglob
          for nb in "${{ inputs.notebooks_dir }}"/*.py; do
            base="$(basename "$nb")"
            echo "Importando $base en ${target_ws_dir}..."
            databricks workspace import "$nb" "${target_ws_dir}/${base}" --language PYTHON --overwrite
          done

      - name: Subir archivos de data y config a DBFS
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"
          databricks fs mkdirs dbfs:/FileStore/data
          databricks fs mkdirs dbfs:/FileStore/jobs_data/raw
          databricks fs mkdirs dbfs:/FileStore/jobs_data/config

          shopt -s nullglob
          for p in "${{ inputs.data_dir }}"/*; do
            name="$(basename "$p")"
            if [ -d "$p" ]; then
              databricks fs cp -r "$p" "dbfs:/FileStore/data/$name" --overwrite
            else
              databricks fs cp "$p" "dbfs:/FileStore/data/$name" --overwrite
            fi
          done

          databricks fs cp "${{ inputs.raw_clientes }}" dbfs:/FileStore/jobs_data/raw/clientes.csv --overwrite
          databricks fs cp "${{ inputs.raw_ventas }}"  dbfs:/FileStore/jobs_data/raw/ventas.json  --overwrite
          databricks fs cp "${{ inputs.config_param_file }}" dbfs:/FileStore/jobs_data/config/parametros.yaml --overwrite

          databricks fs ls dbfs:/FileStore/data || true
          databricks fs ls dbfs:/FileStore/jobs_data/raw || true
          databricks fs ls dbfs:/FileStore/jobs_data/config || true

      - name: Crear o actualizar jobsworkflows en Databricks
        shell: bash
        env:
          JOBS_DIR: ${{ inputs.jobs_dir }}
          JOB_IDS_PATH: .gha/job_ids.json
        run: |
          set -Eeuo pipefail
          mkdir -p .gha
          python3 "$CREATE_SCRIPT"

      - name: Actualizacion de joblist.tmp
        shell: bash
        env:
          WRITE_MAPPING: "false"
          WRITE_FULL_MAPPING: "true"
          FILTER_REPO_ONLY: "true"
        run: |
          set -Eeuo pipefail
          python3 "$JOBLIST_SCRIPT"

      - name: Ejecutar Job(s) y verificar
        shell: bash
        env:
          JOB_IDS_PATH: .gha/job_ids.json
          POLL_SECONDS: "10"
          TAIL_INTERVAL: "10"
          TIMEOUT_SECONDS: "7200"
          LOGS_DBFS_BASE: "dbfs:/cluster-logs"
        run: |
          set -Eeuo pipefail
          python3 "$EXECUTE_SCRIPT"

      # üîπ Paso 7: Descargar outputs desde DBFS
      - name: Descargar outputs de DBFS
        shell: bash
        run: |
          source ~/databricks-env/bin/activate

          OUTPUT_DIR=outputs/
          DBFS_DIR=dbfs:/FileStore/jobs_output/

          # Crear directorio local con Python
          python3 -c "import os; os.makedirs('$OUTPUT_DIR', exist_ok=True)"

          echo "üì• Descargando resultados de $DBFS_DIR a $OUTPUT_DIR"
          databricks fs cp -r "$DBFS_DIR" "$OUTPUT_DIR"
          echo "‚úÖ Outputs listos en $OUTPUT_DIR"

          # Listar archivos usando Python
          echo "üìÑ Listado de archivos en $OUTPUT_DIR:"
          python3 -c "import os; [print(os.path.join(dp, f)) for dp, dn, filenames in os.walk('$OUTPUT_DIR') for f in filenames]"

      - name: Subir outputs como artifacts
        uses: actions/upload-artifact@v4
        with:
          name: job-outputs
          path: outputs/
          retention-days: 7