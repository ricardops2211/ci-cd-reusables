name: verify-databricks

on:
  workflow_call:
    inputs:
      databricks_host:    { type: string,  required: true }
      notebooks_dir:      { type: string,  default: "notebooks" }
      data_dir:           { type: string,  default: "data" }
      raw_clientes:       { type: string,  default: "data/raw/clientes.csv" }
      raw_ventas:         { type: string,  default: "data/raw/ventas.json" }
      config_param_file:  { type: string,  default: "data/config/parametros.yaml" }
      jobs_dir:           { type: string,  default: "jobs" }
      python_scripts_dir: { type: string,  default: "scripts" }
      outputs_dbfs:       { type: string,  default: "dbfs:/FileStore/jobs_output/" }
      artifact_name:      { type: string,  default: "job-outputs" }
      retention_days:     { type: number,  default: 7 }
    secrets:
      DATABRICKS_TOKEN_B64: { required: true }
      # REUSABLES_TOKEN: { required: false }  # <-- solo si el repo reusable es privado

jobs:
  verify:
    runs-on: self-hosted
    env:
      DATABRICKS_HOST: ${{ inputs.databricks_host }}
    steps:
      - uses: actions/checkout@v4   # Esto clona el REPO DEL CALLER por defecto. :contentReference[oaicite:2]{index=2}

      - name: Checkout del repo del reusable (para traer scripts)
        uses: actions/checkout@v4
        with:
          repository: ricardops2211/ci-cd-reusables
          ref: databricks
          path: _reusables
          # token: ${{ secrets.REUSABLES_TOKEN }}   # <-- descomenta si el repo reusable es PRIVADO

      - name: Añadir venv de Databricks al PATH (si existe)
        shell: bash
        run: |
          if [ -d "$HOME/databricks-env/bin" ]; then
            echo "$HOME/databricks-env/bin" >> "$GITHUB_PATH"
          fi

      - name: Decodificar token y exportar a env
        shell: bash
        run: |
          set -Eeuo pipefail
          DBX_TOKEN="$(printf %s "${{ secrets.DATABRICKS_TOKEN_B64 }}" | base64 -d | base64 -d)"
          if [ -z "$DBX_TOKEN" ]; then
            echo "❌ DATABRICKS_TOKEN vacío tras decodificar"; exit 1
          fi
          echo "::add-mask::$DBX_TOKEN"
          echo "DATABRICKS_TOKEN=$DBX_TOKEN" >> "$GITHUB_ENV"

      - name: Sanity check de token
        shell: bash
        run: |
          test -n "$DATABRICKS_TOKEN" || { echo "❌ Token no presente"; exit 1; }

      - name: Resolver ruta de scripts (create/execute/joblist)
        id: resolve_scripts
        shell: bash
        run: |
          set -Eeuo pipefail
          # 1) Intento directo con el input
          CAND="${{ inputs.python_scripts_dir }}"
          if [ -f "$CAND/create-jobdatabricks.py" ] && \
             [ -f "$CAND/execute-jobdatabricks.py" ] && \
             [ -f "$CAND/joblisttmp.py" ]; then
            echo "CREATE_SCRIPT=$CAND/create-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "EXECUTE_SCRIPT=$CAND/execute-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "JOBLIST_SCRIPT=$CAND/joblisttmp.py"         >> "$GITHUB_ENV"
            exit 0
          fi

          # 2) Fallback: scripts dentro del repo del reusable
          if [ -f "_reusables/scripts/create-jobdatabricks.py" ] && \
             [ -f "_reusables/scripts/execute-jobdatabricks.py" ] && \
             [ -f "_reusables/scripts/joblisttmp.py" ]; then
            echo "CREATE_SCRIPT=_reusables/scripts/create-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "EXECUTE_SCRIPT=_reusables/scripts/execute-jobdatabricks.py" >> "$GITHUB_ENV"
            echo "JOBLIST_SCRIPT=_reusables/scripts/joblisttmp.py" >> "$GITHUB_ENV"
            exit 0
          fi

          # 3) Búsqueda global como último recurso
          CREATE="$(find . -maxdepth 6 -type f -name create-jobdatabricks.py | head -n1 || true)"
          EXECUTE="$(find . -maxdepth 6 -type f -name execute-jobdatabricks.py | head -n1 || true)"
          JOBLIST="$(find . -maxdepth 6 -type f -name joblisttmp.py | head -n1 || true)"

          if [ -n "$CREATE" ] && [ -n "$EXECUTE" ] && [ -n "$JOBLIST" ]; then
            echo "CREATE_SCRIPT=$CREATE"   >> "$GITHUB_ENV"
            echo "EXECUTE_SCRIPT=$EXECUTE" >> "$GITHUB_ENV"
            echo "JOBLIST_SCRIPT=$JOBLIST" >> "$GITHUB_ENV"
          else
            echo "❌ No se encontraron scripts requeridos."
            echo "Busqué en '${{ inputs.python_scripts_dir }}', en '_reusables/scripts' y en todo el repo."
            exit 1
          fi

      - name: Importar notebooks en Databricks
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"
          target_ws_dir="/Shared/notebook/py"
          databricks workspace mkdirs "$target_ws_dir"
          shopt -s nullglob
          for nb in "${{ inputs.notebooks_dir }}"/*.py; do
            base="$(basename "$nb")"
            echo "Importando $base en ${target_ws_dir}..."
            databricks workspace import "$nb" "${target_ws_dir}/${base}" --language PYTHON --overwrite
          done

      - name: Subir archivos de data y config a DBFS
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"
          databricks fs mkdirs dbfs:/FileStore/data
          databricks fs mkdirs dbfs:/FileStore/jobs_data/raw
          databricks fs mkdirs dbfs:/FileStore/jobs_data/config

          shopt -s nullglob
          for p in "${{ inputs.data_dir }}"/*; do
            name="$(basename "$p")"
            if [ -d "$p" ]; then
              databricks fs cp -r "$p" "dbfs:/FileStore/data/$name" --overwrite
            else
              databricks fs cp "$p" "dbfs:/FileStore/data/$name" --overwrite
            fi
          done

          databricks fs cp "${{ inputs.raw_clientes }}" dbfs:/FileStore/jobs_data/raw/clientes.csv --overwrite
          databricks fs cp "${{ inputs.raw_ventas }}"  dbfs:/FileStore/jobs_data/raw/ventas.json  --overwrite
          databricks fs cp "${{ inputs.config_param_file }}" dbfs:/FileStore/jobs_data/config/parametros.yaml --overwrite

          databricks fs ls dbfs:/FileStore/data || true
          databricks fs ls dbfs:/FileStore/jobs_data/raw || true
          databricks fs ls dbfs:/FileStore/jobs_data/config || true

      - name: Verificar archivos subidos a DBFS
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"
          PARAM_FILE="dbfs:/FileStore/jobs_data/config/parametros.yaml"
          databricks fs head "$PARAM_FILE" -n 1 >/dev/null 2>&1 || { echo "❌ parametros.yaml no está en DBFS"; exit 1; }
          echo "✅ parametros.yaml existe en DBFS"

      - name: Crear/Actualizar Job(s) en Databricks (UPSERT)
        shell: bash
        env:
          JOBS_DIR: ${{ inputs.jobs_dir }}
          JOB_IDS_PATH: .gha/job_ids.json
        run: |
          set -Eeuo pipefail
          mkdir -p .gha
          python3 "$CREATE_SCRIPT"

      - name: Actualizacion de joblist.tmp
        shell: bash
        env:
          WRITE_MAPPING: "false"
          WRITE_FULL_MAPPING: "true"
          FILTER_REPO_ONLY: "true"
        run: |
          set -Eeuo pipefail
          python3 "$JOBLIST_SCRIPT"

      - name: Ejecutar Job(s) y verificar
        shell: bash
        env:
          JOB_IDS_PATH: .gha/job_ids.json
          POLL_SECONDS: "10"
          TAIL_INTERVAL: "10"
          TIMEOUT_SECONDS: "7200"
          LOGS_DBFS_BASE: "dbfs:/cluster-logs"
        run: |
          set -Eeuo pipefail
          python3 "$EXECUTE_SCRIPT"