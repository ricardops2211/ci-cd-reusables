name: verify-databricks

on:
  workflow_call:
    inputs:
      databricks_host:    { type: string,  required: true }
      notebooks_dir:      { type: string,  default: "notebooks" }
      data_dir:           { type: string,  default: "data" }
      raw_clientes:       { type: string,  default: "data/raw/clientes.csv" }
      raw_ventas:         { type: string,  default: "data/raw/ventas.json" }
      config_param_file:  { type: string,  default: "data/config/parametros.yaml" }
      jobs_dir:           { type: string,  default: "jobs" }
      python_scripts_dir: { type: string,  default: "scripts" }
      outputs_dbfs:       { type: string,  default: "dbfs:/FileStore/jobs_output/" }
      artifact_name:      { type: string,  default: "job-outputs" }
      retention_days:     { type: number,  default: 7 }
    secrets:
      DATABRICKS_TOKEN_B64: { required: true }

jobs:
  verify:
    runs-on: self-hosted
    env:
      DATABRICKS_HOST: ${{ inputs.databricks_host }}
    steps:
      - uses: actions/checkout@v4

      - name: A√±adir venv de Databricks al PATH (si existe)
        shell: bash
        run: |
          if [ -d "$HOME/databricks-env/bin" ]; then
            echo "$HOME/databricks-env/bin" >> "$GITHUB_PATH"
          fi

      - name: Decodificar token y exportar a env
        shell: bash
        run: |
          set -Eeuo pipefail
          DBX_TOKEN="$(printf %s "${{ secrets.DATABRICKS_TOKEN_B64 }}" | base64 -d | base64 -d)"
          if [ -z "$DBX_TOKEN" ]; then
            echo "‚ùå DATABRICKS_TOKEN vac√≠o tras decodificar"; exit 1
          fi
          echo "::add-mask::$DBX_TOKEN"
          echo "DATABRICKS_TOKEN=$DBX_TOKEN" >> "$GITHUB_ENV"

      - name: Sanity check de token
        shell: bash
        run: |
          test -n "$DATABRICKS_TOKEN" || { echo "‚ùå Token no presente"; exit 1; }

      # Importar notebooks
      - name: Importar notebooks en Databricks
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"
          target_ws_dir="/Shared/notebook/py"
          databricks workspace mkdirs "$target_ws_dir"
          shopt -s nullglob
          for nb in "${{ inputs.notebooks_dir }}"/*.py; do
            base="$(basename "$nb")"
            echo "Importando $base en ${target_ws_dir}..."
            databricks workspace import "$nb" "${target_ws_dir}/${base}" --language PYTHON --overwrite
          done

      # Subir data/config a DBFS
      - name: Subir archivos de data y config a DBFS
        shell: bash
        run: |
          set -Eeuo pipefail
          [ -f "$HOME/databricks-env/bin/activate" ] && source "$HOME/databricks-env/bin/activate"

          databricks fs mkdirs dbfs:/FileStore/data
          databricks fs mkdirs dbfs:/FileStore/jobs_data/raw
          databricks fs mkdirs dbfs:/FileStore/jobs_data/config

          shopt -s nullglob
          for p in "${{ inputs.data_dir }}"/*; do
            name="$(basename "$p")"
            if [ -d "$p" ]; then
              echo "üì§ Carpeta $name ‚Üí DBFS:/FileStore/data/$name"
              databricks fs cp -r "$p" "dbfs:/FileStore/data/$name" --overwrite
            else
              echo "üì§ Archivo $name ‚Üí DBFS:/FileStore/data/$name"
              databricks fs cp "$p" "dbfs:/FileStore/data/$name" --overwrite
            fi
          done

          databricks fs cp "${{ inputs.raw_clientes }}" dbfs:/FileStore/jobs_data/raw/clientes.csv --overwrite
          databricks fs cp "${{ inputs.raw_ventas }}"  dbfs:/FileStore/jobs_data/raw/ventas.json  --overwrite
          databricks fs cp "${{ inputs.config_param_file }}" dbfs:/FileStore/jobs_data/config/parametros.yaml --overwrite

          echo "üìÇ Contenido de dbfs:/FileStore/data"
          databricks fs ls dbfs:/FileStore/data || true
          echo "üìÇ Contenido de dbfs:/FileStore/jobs_data/raw"
          databricks fs ls dbfs:/FileStore/jobs_data/raw || true
          echo "üìÇ Contenido de dbfs:/FileStore/jobs_data/config"
          databricks fs ls dbfs:/FileStore/jobs_data/config || true

      - name: Crear/Actualizar Job(s) en Databricks (UPSERT)
        shell: bash
        env:
          JOBS_DIR: ${{ inputs.jobs_dir }}
          JOB_IDS_PATH: .gha/job_ids.json
        run: |
          set -Eeuo pipefail
          mkdir -p .gha
          python3 "${{ inputs.python_scripts_dir }}/create-jobdatabricks.py"

      - name: Actualizacion de joblist.tmp
        shell: bash
        env:
          WRITE_MAPPING: "false"
          WRITE_FULL_MAPPING: "true"
          FILTER_REPO_ONLY: "true"
        run: |
          set -Eeuo pipefail
          python3 "${{ inputs.python_scripts_dir }}/joblisttmp.py"

      - name: Ejecutar Job(s) y verificar
        shell: bash
        env:
          JOB_IDS_PATH: .gha/job_ids.json
          POLL_SECONDS: "10"
          TAIL_INTERVAL: "10"
          TIMEOUT_SECONDS: "7200"
          LOGS_DBFS_BASE: "dbfs:/cluster-logs"
        run: |
          set -Eeuo pipefail
          python3 "${{ inputs.python_scripts_dir }}/execute-jobdatabricks.py"

      # üîπ Paso 7: Descargar outputs desde DBFS
      - name: Descargar outputs de DBFS
        shell: bash
        env:
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          OUTPUT_DIR=outputs/
          DBFS_DIR=dbfs:/FileStore/jobs_output/

          # Crear directorio local con Python
          python3 -c "import os; os.makedirs('$OUTPUT_DIR', exist_ok=True)"

          echo "üì• Descargando resultados de $DBFS_DIR a $OUTPUT_DIR"
          databricks fs cp -r "$DBFS_DIR" "$OUTPUT_DIR"
          echo "‚úÖ Outputs listos en $OUTPUT_DIR"

          # Listar archivos usando Python
          echo "üìÑ Listado de archivos en $OUTPUT_DIR:"
          python3 -c "import os; [print(os.path.join(dp, f)) for dp, dn, filenames in os.walk('$OUTPUT_DIR') for f in filenames]"

      - name: Subir outputs como artifacts
        uses: actions/upload-artifact@v4
        with:
          name: job-outputs
          path: outputs/
          retention-days: 7
